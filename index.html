<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>LIS500 Project 3 Statement</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    
    <!-- Header Section -->
    <header class="site-header">
      <h1>LIS500 Project 3</h1>
      <nav>
        <ul class="nav-menu">
          <li><a href="index.html">Home</a></li>
          <li><a href="machine.html">Our Machine</a></li>
        </ul>
      </nav>
    </header>

    <!-- Main Content -->
    <main class="main-content">
      <section class="intro-section">
        <h2>Introduction</h2>
        <p>This is a voice classifier that is browser-based and can identify interjections—short bursts of emotion like "wow!", "ugh!", or "huh?"—and classify them as expressions of positiveness, negativity, or wonder. The app uses a Teachable Machine sound model that has been trained to identify these speech indicators and automatically respond with the related emoji. 
          Technically speaking, this was built with the ml5.js library, a wrapper for TensorFlow.js that's dedicated to taking machine learning into the browser. The model was trained using Google's Teachable Machine, which is a low-code/no-code tool that allows users to develop sound or image classifiers by training it on labeled examples.
          The purpose of this activity was not only to understand how machine learning models function, but to also critique their ethical and cultural implications—particularly through the lens of Joy Buolamwini’s Unmasking AI. This juxtaposition—building a model while reflecting on the dangers of biased models—was at the heart of the learning experience.</p>
        <button class="button"><a href="https://thecodingtrain.com/tracks/teachable-machine/teachable-machine/3-sound-classification">Learn More</a></button>
      </section>
      <img src = "images/machinemodel.png" height = "400" width = "600">
      <section class="person">
        <div class="persontext">
          <h2>Reflections on Unmasking AI</h2>
          <p>Joy Buolamwini's book, Unmasking AI, is a wake-up call for anyone implementing machine learning. Who is this for? Who gets left behind? And who could get harmed by the way this model acts—or doesn't act?
            Buolamwini’s story about how facial recognition systems failed to recognize her as a Black woman until she wore a white mask is chilling. It’s not just a bug—it’s a signal that the system wasn’t designed with her in mind. In our case, the stakes are lower: our classifier only displays emojis. But even this trivial system reveals cracks in fairness. 
            When it was tested on other voices—friends with other accents, ways of speaking, or sound quality—the classifier generated incorrect or unreliable results.</p>
        </div>
      </section>
      <section class="person">
        <div class="persontext">
          <h2>Ethical Implications</h2>
          <p>Even a benevolent classifier poses larger ethical concerns. If this model were implemented in an actual application—e.g., tracking mood in a workplace or classroom? Classifying someone's tone as "negative" or "confused" could have significant implications, especially for those whose speech patterns don't meet the model's definition of "normal."
              Buolamwini warns us not to act as though AI is magic. Algorithms are too frequently imbued with powers they do not require. The more we understand just how fragile and human-dependent these systems are, the more responsibly we can use them.
              Our take-away is that the responsibility for the limitations of models lies with the developers. Transparency matters. Users should know what a model was trained on, what it cannot do, and who it may not be able to work for. This shift in perspective—shifting from building "solutions" to building responsible systems—is probably the most evocative Unmasking AI lesson.</p>
         <p>The most humbling part of this activity was realizing how much “intelligence” is attributed to models that are really just guessing based on surface-level features. My interjection classifier doesn’t “understand” tone or emotion—it pattern-matches against a narrow dataset. That illusion of understanding is dangerous.
          Buolamwini’s call for a “technoethical” future—one grounded in justice, inclusion, and humility—hits differently once you’ve built a model yourself. You begin to see how quickly power accumulates around algorithms, and how easily harm can follow.
          In my small way, this project is a gesture toward that future. It’s imperfect, but it’s aware of its imperfection. And maybe that’s the first step toward responsible AI: knowing what we don’t know, and designing accordingly.</p>
          <p>As Buolamwini explains, the problem with the coded gaze is not just bias, but invisibility. Those who don’t conform to the system’s expectations are often erased—not seen, not heard, not registered. This is exactly what happened when Buolamwini’s face was undetected by facial recognition systems unless she wore a white mask. In my case, users with unexpected vocal patterns may not be classified at all. The system gives them a question mark emoji (❓)—seeing uncertainty, but also, incidentally, a sort of dismissal. What does it even say when a system literally can't hear your voice? This relates back to a second thread of Unmasking AI: intersectionality. Buolamwini employs the writing of Kimberlé Crenshaw to illustrate how systems often fail most dramatically at the point of intersection of race, gender, and other identity categories. A voice classifier that "works pretty well" for a certain kind of speaker will totally fail for another—especially if their voice fails to conform to the unstated "norm" built into the training data. My little project demonstrates this miniaturized. A friend whose accent is thicker tried to make it work, and the hits were patchy. Her "wow" got marked as "negativity" sometimes. It's not just a defect of the data—this is a symbolic one. The system is getting emotion wrong and, in doing so, getting identity wrong.</p>
        </div>
      </section>
      <section class="person">
        <div class="persontext">
          <h2>What Was Learned</h2>
          <p>One of the most compelling conclusions from Unmasking AI is that exclusion is inscribed silently into the tools we make. As we struggled to train my own classifier, we appreciated how easy it is to overlook whose voices are being heard—and whose aren't. Buolamwini's example about facial recognition not recognizing Black women is a reminder that a lack of representation isn't technical—a human issue.
We also very much resonated with the idea that transparency is an ethical imperative. Machine learning models can be black boxes, but they're designed by humans who are making choices—what data to train on, how to label it, and which outcomes to optimize for. Those choices shape the physical world.
As we worked on this project, we kept returning to Buolamwini's call to be "an engineer of equity." Even with a playful model like my own, we wanted to think carefully about the assumptions programmed into the system. My model is for whom? What are the potential pitfalls? And how can we communicate its limitations clearly?
Last but not least, Unmasking AI taught us that ethical design isn't feature add-ons or checkboxing—it's designing with care, listening to impacted communities, and recognizing that no technology is ever neutral.</p>
      <p>Accountability means knowing your limits: Buolamwini writes that “we cannot outsource our responsibilities to machines.” That hit home for us. Even if this classifier is “just for fun,” it’s still something we made and released into the world. We need to stand behind its choices—and, just as importantly, be honest about its shortcomings.</p> 
        </div>
      </section>
    </main>

    <!-- Footer -->
    <footer class="site-footer">
      <p>&copy; 2025 Arin Chandra, Yalitza Lopez, Aleena Zehra. All rights reserved.</p>
    </footer>

  </body>
</html>
