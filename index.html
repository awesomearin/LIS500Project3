<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>LIS500 Project 3 Statement</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    
    <!-- Header Section -->
    <header class="site-header">
      <h1>LIS500 Project 3</h1>
      <nav>
        <ul class="nav-menu">
          <li><a href="index.html">Home</a></li>
          <li><a href="machine.html">Our Machine</a></li>
        </ul>
      </nav>
    </header>

    <!-- Main Content -->
    <main class="main-content">
      <section class="intro-section">
        <h2>Introduction</h2>
        <p>This is a voice classifier that is browser-based and can identify interjections‚Äîshort bursts of emotion like "wow!", "ugh!", or "huh?"‚Äîand classify them as expressions of positiveness, negativity, or wonder. The app uses a Teachable Machine sound model that has been trained to identify these speech indicators and automatically respond with the related emoji. 
          Technically speaking, this was built with the ml5.js library, a wrapper for TensorFlow.js that's dedicated to taking machine learning into the browser. The model was trained using Google's Teachable Machine, which is a low-code/no-code tool that allows users to develop sound or image classifiers by training it on labeled examples.
          The purpose of this activity was not only to understand how machine learning models function, but to also critique their ethical and cultural implications‚Äîparticularly through the lens of Joy Buolamwini‚Äôs Unmasking AI. This juxtaposition‚Äîbuilding a model while reflecting on the dangers of biased models‚Äîwas at the heart of the learning experience.</p>
       <iframe width = "720" height = "480" src = "https://youtube.com/embed/Dk5YDQvtR_M?si=LSkaFppyp-kh5tm4"> </iframe>
      </section>
      <img src = "images/machinemodel.png" height = "400" width = "600">
      <section class="person">
        <div class="persontext">
          <h2>Statement on the Project</h2>
          <p>This project included multiple stages: collecting and labeling audio samples, training the model, integrating the model into a JavaScript-based web app using the ml5.js library, and designing a user-friendly frontend experience using HTML and CSS. The classifier listens for audio inputs from the user's microphone and uses the trained model to classify those inputs in real time. The result is represented by an emoji on the screen that visually aligns with the emotional tone of the sound: üòÄ for positivity, üò• for negativity, and ü§î for bewilderment.</p>
          <p>Throughout development, design choices were influenced by the need for clarity and accessibility. From the color palette to font selection and layout structure, everything was made to be mobile-friendly and visually intuitive, providing users with a smooth interactive experience. The inclusion of visual feedback via emojis not only makes the classification results more tangible but also serves as a commentary on how algorithms simplify complex human emotions.</p>
       <p>This project served as both a technical experiment and a critical reflection on the implications of machine learning in understanding human expression. It raises questions about how well algorithms can capture nuance, what cultural assumptions are embedded in emotional labeling, and whether such systems empower or reduce the people they interpret. This concern is at the heart of Joy Buolamwini's Unmasking AI, where she explores how AI can perpetuate inequality under the guise of objectivity.</p>
        <p>Ultimately, the project highlights both the promise and the peril of AI. On one hand, it empowers users to interact with technology in creative, playful ways. On the other hand, it reveals the reductive logic that often underpins machine learning. Our classifier is not just a technical tool; it is a cultural artifact that reflects contemporary hopes and fears about AI. By situating it within critical frameworks, we hope to contribute to a more nuanced and responsible conversation about algorithmic design.</p>
        </div>
      </section>
      <section class="person">
        <div class="persontext">
          <h2>Ethical Implications</h2>
          <p>Even a benevolent classifier poses larger ethical concerns. If this model were implemented in an actual application‚Äîe.g., tracking mood in a workplace or classroom? Classifying someone's tone as "negative" or "confused" could have significant implications, especially for those whose speech patterns don't meet the model's definition of "normal."
              Buolamwini warns us not to act as though AI is magic. Algorithms are too frequently imbued with powers they do not require. The more we understand just how fragile and human-dependent these systems are, the more responsibly we can use them.
              Our take-away is that the responsibility for the limitations of models lies with the developers. Transparency matters. Users should know what a model was trained on, what it cannot do, and who it may not be able to work for. This shift in perspective‚Äîshifting from building "solutions" to building responsible systems‚Äîis probably the most evocative Unmasking AI lesson.</p>
        <p>We also considered how this project relates to the ideas in Booten‚Äôs essay, ‚Äú#All Lives Matter,‚Äù which critiques the universalizing tendencies of digital platforms that flatten context and nuance. Our classifier could be seen as replicating this issue by turning human emotion into a small set of machine-interpretable labels. While the goal is not to erase complexity, the act of classification inherently reduces ambiguity. This simplification mirrors how platforms sometimes impose uniform meanings onto diverse expressions‚Äîa central concern in Booten‚Äôs analysis. Our project, in trying to be inclusive and accessible, still reflects our positionality and cultural lens.</p>
          <p>In reflecting on these questions, we were also inspired by Ruha Benjamin's Race After Technology. Benjamin argues that "the New Jim Code" operates through supposedly neutral algorithms that reinforce systemic racism. Her work pushed us to think critically about the seemingly harmless nature of our project. While emoji feedback may seem trivial, the act of labeling emotion has historical and political weight. Our classifier cannot escape the cultural baggage of classification itself.</p>
          <p>The most humbling part of this activity was realizing how much ‚Äúintelligence‚Äù is attributed to models that are really just guessing based on surface-level features. Our interjection classifier doesn‚Äôt ‚Äúunderstand‚Äù tone or emotion‚Äîit pattern-matches against a narrow dataset. That illusion of understanding is dangerous.
          Buolamwini‚Äôs call for a ‚Äútechnoethical‚Äù future‚Äîone grounded in justice, inclusion, and humility‚Äîhits differently once you‚Äôve built a model yourself. You begin to see how quickly power accumulates around algorithms, and how easily harm can follow.
          In our small way, this project is a gesture toward that future. It‚Äôs imperfect, but it‚Äôs aware of its imperfection. And maybe that‚Äôs the first step toward responsible AI: knowing what we don‚Äôt know, and designing accordingly.</p>
          <p>As Buolamwini explains, the problem with the coded gaze is not just bias, but invisibility. Those who don‚Äôt conform to the system‚Äôs expectations are often erased‚Äînot seen, not heard, not registered. This is exactly what happened when Buolamwini‚Äôs face was undetected by facial recognition systems unless she wore a white mask. In our case, users with unexpected vocal patterns may not be classified at all. The system gives them a question mark emoji (‚ùì)‚Äîseeing uncertainty, but also, incidentally, a sort of dismissal. What does it even say when a system literally can't hear your voice? This relates back to a second thread of Unmasking AI: intersectionality. Buolamwini employs the writing of Kimberl√© Crenshaw to illustrate how systems often fail most dramatically at the point of intersection of race, gender, and other identity categories. A voice classifier that "works pretty well" for a certain kind of speaker will totally fail for another‚Äîespecially if their voice fails to conform to the unstated "norm" built into the training data. Our little project demonstrates this miniaturized. A friend whose accent is thicker tried to make it work, and the hits were patchy. Her "wow" got marked as "negativity" sometimes. It's not just a defect of the data‚Äîthis is a symbolic one. The system is getting emotion wrong and, in doing so, getting identity wrong.</p>
        </div>
      </section>
      <section class="person">
        <div class="persontext">
          <h2>What Was Learned</h2>
          <p>One of the most compelling conclusions from Unmasking AI is that exclusion is inscribed silently into the tools we make. As we struggled to train our own classifier, we appreciated how easy it is to overlook whose voices are being heard‚Äîand whose aren't. Buolamwini's example about facial recognition not recognizing Black women is a reminder that a lack of representation isn't technical‚Äîa human issue.
We also very much resonated with the idea that transparency is an ethical imperative. Machine learning models can be black boxes, but they're designed by humans who are making choices‚Äîwhat data to train on, how to label it, and which outcomes to optimize for. Those choices shape the physical world.
As we worked on this project, we kept returning to Buolamwini's call to be "an engineer of equity." Even with a playful model like our own, we wanted to think carefully about the assumptions programmed into the system. Our model is for whom? What are the potential pitfalls? And how can we communicate its limitations clearly?
Last but not least, Unmasking AI taught us that ethical design isn't feature add-ons or checkboxing‚Äîit's designing with care, listening to impacted communities, and recognizing that no technology is ever neutral.</p>
      <p>Accountability means knowing your limits: Buolamwini writes that ‚Äúwe cannot outsource our responsibilities to machines.‚Äù That hit home for us. Even if this classifier is ‚Äújust for fun,‚Äù it‚Äôs still something we made and released into the world. We need to stand behind its choices‚Äîand, just as importantly, be honest about its shortcomings.</p> 
        <p>Going forward, improvements might include a broader emotional vocabulary, multilingual support, or cultural customization to better reflect different emotional registers. But even as we improve the tech, we must remain grounded in the ethical insights offered by Buolamwini, Booten, and other critical scholars. Their work reminds us that building responsibly means thinking beyond functionality‚Äîtoward fairness, context, and justice.
</p>
          
        </div>

      </section>
      <button class="button"><a href="https://thecodingtrain.com/tracks/teachable-machine/teachable-machine/3-sound-classification">Learn More</a></button>

    </main>

    <!-- Footer -->
    <footer class="site-footer">
      <p>&copy; 2025 Arin Chandra, Yalitza Lopez, Aleena Zehra. All rights reserved.</p>
    </footer>

  </body>
</html>
